
# 因果推断方法小结1
因果推断主要分类两类统计模型，一个是以Rubin为代表的潜在结果模型，另一个是以Pearl为代表的因果网络模型。
## 潜在结果模型
大多数潜在结果模型方法都属于计量经济学范畴，常用于评估政策效应，具体有以下几类。
### 断点回归

>断点回归的主要原理是：存在一个变量，如果该变量大于这个临界值时，接受处置效应，小于临界值时，不接受处置效应，可以视作是对照组。经典案例有Chen et al.（2013）观察到的一条天然的分割线——秦岭淮河线，即以秦岭淮河线作为临界线，其中：淮河以北地区，政府用燃煤的方式提供暖气，视为接受政策干预；而淮河以南地区，并没有供应暖气，视为未接受政策干预。淮河两岸十分接近的两个地区，理论上其他各变量可以看作是连续的，也就是说其他的影响变量在南北两岸没有较大差异，而南北两岸唯一的区别就是有没有通过燃煤供暖，所以淮河以南可以作为很好的对照组，通过与实验组比较，识别政策干预效应。 

断点回归主要分为两类：
- 确定型（Sharp RDD）实线
- 模糊型（Fuzzy RDD）虚线

<img src="https://github.com/guanyu0316/all-images/blob/main/RDD4.png?raw=true" width = "350" height = "300" alt="" align=center/>


#### Sharp RDD 模型原理
<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+D%0A" 
alt="D
">为处理变量，是分组变量<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">的确定函数，<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">的断点为c，则：

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+D_%7Bi%7D%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0A1+%26+%5Ctext+%7B+if+%7D++x_%7Bi%7D+%5Cgeqslant+c+%5C%5C%0A0+%26+%5Ctext+%7B+if+%7D++x_%7Bi%7D%3Cc%0A%5Cend%7Barray%7D%5Cright." 
alt="D_{i}=\left\{\begin{array}{ll}
1 & \text { if }  x_{i} \geqslant c \\
0 & \text { if }  x_{i}<c
\end{array}\right.">
 
假设在实验前<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Y" 
alt="Y">与<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">呈线性相关，<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+y_%7Bi%7D%3D%5Calpha%2B%5Cbeta+x_%7Bi%7D%2B%5Cvarepsilon_%7Bi%7D+%5Cquad%28i%3D1%2C+%5Ccdots%2C+n%29" 
alt="y_{i}=\alpha+\beta x_{i}+\varepsilon_{i} \quad(i=1, \cdots, n)">，确定型断点回归可理解为这个线性关系在<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X%3Dc" 
alt="X=c">存在一个跳跃点，如下图所示。

<img src="https://github.com/guanyu0316/all-images/blob/main/Sharp%20RDD.png?raw=true" width = "350" height = "300" alt="" align=center />

我们可以引入虚拟变量及交互项来表示这个回归方程，以在断点两侧产生不同的截距项和斜率。

  <img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+y_%7Bi%7D%3D%5Calpha%2B%5Cbeta%5Cleft%28x_%7Bi%7D-c%5Cright%29%2B%5Cdelta+D_%7Bi%7D%2B%5Cgamma%5Cleft%28x_%7Bi%7D-c%5Cright%29+D_%7Bi%7D%2B%5Cvarepsilon_%7Bi%7D+" 
alt="y_{i}=\alpha+\beta\left(x_{i}-c\right)+\delta D_{i}+\gamma\left(x_{i}-c\right) D_{i}+\varepsilon_{i} ">
  
估计量<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7B%5Cdelta%7D" 
alt="\hat{\delta}">就是我们要的平均因果效应。

但存在以下问题：
- 非线性关系
- 断点回归相当于局部的随机试验，原则上只能使用断点附近的样本

解决方案：
- 添加高次项并限定样本取值范围
  
<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+y_%7Bi%7D%3D%5Calpha%2B%5Cbeta_%7B1%7D%5Cleft%28x_%7Bi%7D-c%5Cright%29%2B%5Cdelta+D_%7Bi%7D%2B%5Cgamma_%7B1%7D%5Cleft%28x_%7Bi%7D-c%5Cright%29+D_%7Bi%7D+%2B%5Cbeta_%7B2%7D%5Cleft%28x_%7Bi%7D-c%5Cright%29%5E%7B2%7D%2B%5Cgamma_%7B2%7D%5Cleft%28x_%7Bi%7D-c%5Cright%29%5E%7B2%7D+D_%7Bi%7D%2B%5Cvarepsilon_%7Bi%7D+%5Cquad%28c-h%3Cx%3Cc%2Bh%29" 
alt="y_{i}=\alpha+\beta_{1}\left(x_{i}-c\right)+\delta D_{i}+\gamma_{1}\left(x_{i}-c\right) D_{i} +\beta_{2}\left(x_{i}-c\right)^{2}+\gamma_{2}\left(x_{i}-c\right)^{2} D_{i}+\varepsilon_{i} \quad(c-h<x<c+h)">

- 使用非参数方法核回归
#### Fuzzy RDD 模型原理
在模糊断点回归中，处理变量<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+D" 
alt="D">并不完全由分组变量<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">决定，即使<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+x%3Ec" 
alt="x>c">，个体也不一定接受处理，只是有一个接受处理的概率，这个处理概率在断点处存在跳跃，因此精确断点回归可以看做是模糊断点回归的极限情形。

<img src="https://github.com/guanyu0316/all-images/blob/main/Fuzzy2.png?raw=true" width = "400" height = "300" alt="" align=center />

模糊断点回归需要可忽略性假定，经过推导可得：

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+ACE+%5Cequiv+E%5Cleft%5B%5Cleft%28y_%7B1%7D-y_%7B0%7D%5Cright%29+%5Cmid+x%3Dc%5Cright%5D%3D%5Cfrac%7B%5Clim+_%7Bx+%5Cdownarrow+c%7D+E%28y+%5Cmid+x%29-%5Clim+_%7Bx+%5Cuparrow+c%7D%28y+%5Cmid+x%29%7D%7B%5Clim+_%7Bx+%5Cdownarrow+c%7D+E%28D+%5Cmid+x%29-%5Clim+_%7Bx+%5Cuparrow+c%7D+E%28D+%5Cmid+x%29%7D" 
alt="ACE \equiv E\left[\left(y_{1}-y_{0}\right) \mid x=c\right]=\frac{\lim _{x \downarrow c} E(y \mid x)-\lim _{x \uparrow c}(y \mid x)}{\lim _{x \downarrow c} E(D \mid x)-\lim _{x \uparrow c} E(D \mid x)}">

其中分子是精确断点回归的处理效应，分母是倾向得分在断点处的跳跃<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+b-a" 
alt="b-a">，在精确断点回归中这个跳跃为1，因此精确断点回归是这个公式的极限情况。


### 双重差分
首先引入一个例子：
>我们想考察垃圾焚烧炉对房价的影响，于是我们设计如下变量。
>
><img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext+%7B+nearinc+%7D%3D+%5Cbegin%7Bcases%7D1+%26+%5Ctext+%7B+The+%5C%3Bdistance++%5C%3Bof++%5C%3Ba+%5C%3B+house+%5C%3B+from+%5C%3B+a+%5C%3B+garbage+%5C%3B+incinerator+%7D%3C1+%5Cmathrm%7Bkm%7D+%5C%5C+0+%26+%5Ctext+%7B+The++%5C%3Bdistance+%5C%3B+of++%5C%3Ba++%5C%3Bhouse+%5C%3B+from++%5C%3Ba+%5C%3B+garbage+%5C%3B+incinerator+%7D+%5Cgeq+1+%5Cmathrm%7Bkm%7D%5Cend%7Bcases%7D" 
alt="\text { nearinc }= \begin{cases}1 & \text { The \;distance  \;of  \;a \; house \; from \; a \; garbage \; incinerator }<1 \mathrm{km} \\ 0 & \text { The  \;distance \; of  \;a  \;house \; from  \;a \; garbage \; incinerator } \geq 1 \mathrm{km}\end{cases}">


很显然，如果我们利用房价Price对nearinc直接进行一元回归，存在非常严重的遗漏变量偏差。这时可以采用双重差分的方法，如果这些**遗漏变量对因变量影响不随时间的变化而变化**。
1997（t=0）之前，政府没有修建垃圾焚烧炉，1997年，政府修建了垃圾焚烧炉。那么在1997年时，靠近垃圾焚烧炉的房子与远离的房子房价之间本身就存在着价差，这个价差是由于其他遗漏因素导致的，由遗漏因素导致的价差在1997年与2003年是一样的，因此03年的价差减去这个固有的价差，就是处置效应。

下图可以很清晰地说明双重差分的原理：

<img src="https://github.com/guanyu0316/all-images/blob/main/DID2.png?raw=true" width = "340" height = "300" alt="" align=center />

我们可以用回归来计算处置效应：

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+y_%7Bi+t%7D%3D%5Cbeta_%7B1%7D%2B%5Cbeta_%7B2%7D+T_%7Bt%7D%2B%5Cbeta_%7B3%7D+D_%7Bi%7D%2B%5Cbeta_%7B4%7D+T_%7Bt%7D+%5Ctimes+D_%7Bi%7D%2B%5Cvarepsilon_%7Bi+t%7D%2C+%5Cquad+t%3D1%2C2" 
alt="y_{i t}=\beta_{1}+\beta_{2} T_{t}+\beta_{3} D_{i}+\beta_{4} T_{t} \times D_{i}+\varepsilon_{i t}, \quad t=1,2">

其中<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+T" 
alt="T">表示时间，为定性二元变量，<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+D" 
alt="D">为我们要考察的自变量，在上边的例子里，<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+D" 
alt="D">就是nearinc。

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cleft%28y_%7Bi+2%7D+%5Cmid+D_%7Bi%7D%3D1%5Cright%29-%5Cleft%28y_%7Bi+1%7D+%5Cmid+D_%7Bi%7D%3D1%5Cright%29%3D%5Cleft%28%5Cbeta_%7B1%7D%2B%5Cbeta_%7B2%7D%2B%5Cbeta_%7B3%7D%2B%5Cbeta_%7B4%7D%5Cright%29-%5Cleft%28%5Cbeta_%7B1%7D%2B%5Cbeta_%7B3%7D%5Cright%29%3D%5Cbeta_%7B2%7D%2B%5Cbeta_%7B4%7D" 
alt="\left(y_{i 2} \mid D_{i}=1\right)-\left(y_{i 1} \mid D_{i}=1\right)=\left(\beta_{1}+\beta_{2}+\beta_{3}+\beta_{4}\right)-\left(\beta_{1}+\beta_{3}\right)=\beta_{2}+\beta_{4}">

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cleft%28y_%7Bi+2%7D+%5Cmid+D_%7Bi%7D%3D0%5Cright%29-%5Cleft%28y_%7Bi+1%7D+%5Cmid+D_%7Bi%7D%3D0%5Cright%29%3D%5Cleft%28%5Cbeta_%7B1%7D%2B%5Cbeta_%7B2%7D%5Cright%29-%5Cbeta_%7B1%7D%3D%5Cbeta_%7B2%7D" 
alt="\left(y_{i 2} \mid D_{i}=0\right)-\left(y_{i 1} \mid D_{i}=0\right)=\left(\beta_{1}+\beta_{2}\right)-\beta_{1}=\beta_{2}">

<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cleft%5B%5Cleft%28y_%7Bi+2%7D+%5Cmid+D_%7Bi%7D%3D1%5Cright%29-%5Cleft%28y_%7Bi+1%7D+%5Cmid+D_%7Bi%7D%3D1%5Cright%29%5Cright%5D-%5Cleft%5B%5Cleft%28y_%7Bi+2%7D+%5Cmid+D_%7Bi%7D%3D0%5Cright%29-%5Cleft%28y_%7Bi+1%7D+%5Cmid+D_%7Bi%7D%3D0%5Cright%29%5Cright%5D%3D%5Cbeta_%7B4%7D" 
alt="\left[\left(y_{i 2} \mid D_{i}=1\right)-\left(y_{i 1} \mid D_{i}=1\right)\right]-\left[\left(y_{i 2} \mid D_{i}=0\right)-\left(y_{i 1} \mid D_{i}=0\right)\right]=\beta_{4}">

因此<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbeta_%7B4%7D" 
alt="\beta_{4}">的估计值就是我们要计算的因果效应。

值得注意的是DID仅适用于**面板数据**。不过，在某些特殊的情景下，截面数据通过巧妙的构造也是可以运用DID的。

### 倾向得分匹配
在介绍倾向得分匹配之前，首先要了解匹配法，匹配法的思路是对每一个个体匹配一个具有相同或相近协变量取值的个体集合, 使得匹配得到的数据在处理组和对照组有相同的协变量分布, 然后根据匹配数据推断因果作用。匹配法可以分为协变量匹配（维数灾难）和倾向得分匹配。匹配法的技术细节有是否放回、是否允许并列、是否一对多匹配。
#### 倾向得分匹配的原理、假定
倾向得分的定义是<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cpi%28V%29%3D%5Coperatorname%7Bpr%7D%28X%3D1+%5Cmid+V%29" 
alt="\pi(V)=\operatorname{pr}(X=1 \mid V)">，倾向得分匹配之所以可以使用，是基于以下原理：
> 如果给定协变量<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+V" 
alt="V">时可忽略性成立，即<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Y_%7Bx%7D+%5Cperp+X+%5Cmid+V" 
alt="Y_{x} \perp X \mid V">且<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+0%3C%5Cp%28X%3D1+%5Cmid+V%29%3C1" 
alt="0<\p(X=1 \mid V)<1">，那么给定倾向得分时<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cpi%28V%29" 
alt="\pi(V)">时，可忽略假定也成立，即<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Y_%7Bx%7D+%5Cperp+X+%5Cmid+%5Cpi%28V%29" 
alt="Y_{x} \perp X \mid \pi(V)">且<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+0%3Cp%28X%3D1+%5Cmid+%5Cpi%28V%29%5C%29%29%3C1" 
alt="0<p(X=1 \mid \pi(V)\))<1">。

另外，倾向得分匹配还有一个重叠假定要求，即控制组和处理组的倾向得分取值范围要有重叠的部分。
#### 倾向得分匹配的步骤
1. 选择协变量<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+x_%7Bi%7D" 
alt="x_{i}">。
2. 估计倾向得分，一般使用logit回归。Rosenbaum and Rubin（1985）建议使用形式灵活的 logit模型，比如包括协变量的高次项与交互项。
3. 进行倾向得分匹配。具体匹配原则有：
   - 近邻匹配：K近邻、限制倾向得分的绝对距离（卡尺匹配、半径匹配）、卡尺内最近邻
   - 整体匹配：每位个体的匹配结果为不同组的全部个体，只是根据个体距离不同给予不同的权重，如果使用核函数来计算权重称之为核匹配。除此之外还有局部线性回归匹配、样条匹配等。
4. 根据匹配后的样本计算平均处理效应。实验组的平均处理效应为：
   
   <img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cwidehat%7B%5Cmathrm%7BATT%7D%7D%3D%5Cfrac%7B1%7D%7BN_%7B1%7D%7D+%5Csum_%7Bi+%3B+D_%7Bi%7D%3D1%7D%5Cleft%28y_%7Bi%7D-%5Chat%7By%7D_%7B0+i%7D%5Cright%29" 
alt="\widehat{\mathrm{ATT}}=\frac{1}{N_{1}} \sum_{i ; D_{i}=1}\left(y_{i}-\hat{y}_{0 i}\right)">

   其中<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+N_%7B1%7D%3D%5Csum_%7Bi%7D+D_%7Bi%7D" 
alt="N_{1}=\sum_{i} D_{i}">为处理组个体数，<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Csum_%7Bi%3A+D_%7Bi%7D%3D1%7D" 
alt="\sum_{i: D_{i}=1}">表示只对处理组进行加总。当然也可以计算控制组的或者整个样本的ATT。
5. 平衡性检验
如果倾向得分匹配法能够很好地匹配样本，那么各个协变量在处理组和控制组分布的差异应当不大，这称之为“平衡性”。我们可以通过比较匹配后各个协变量在两组的均值、中位数等指标来进行初步的平衡性检验，更为严谨的办法是考察每一个协变量的标准化差距，即<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cfrac%7B%5Cleft%7C%5Cbar%7Bx%7D_%7B%5Ctext+%7Btreat+%7D%7D-%5Cbar%7Bx%7D_%7B%5Ctext+%7Bcontrol+%7D%7D%5Cright%7C%7D%7B%5Csqrt%7B%5Cleft%28s_%7Bx%2C+%5Ctext+%7B+treat+%7D%7D%5E%7B2%7D%2Bs_%7Bx%2C+%5Ctext+%7B+control+%7D%7D%5E%7B2%7D%5Cright%29+%2F+2%7D%7D" 
alt="\frac{\left|\bar{x}_{\text {treat }}-\bar{x}_{\text {control }}\right|}{\sqrt{\left(s_{x, \text { treat }}^{2}+s_{x, \text { control }}^{2}\right) / 2}}">，一般要求此标准化差距不超过10%，如果超过了，应更换匹配方法，或者在估计倾向得分时，引入更多协变量及其非线性项。
### 内生性与工具变量
#### 内生性
内生性指的是解释变量与误差项相关。注意在pearl因果图理论中，内生变量的定义有所不同，因果图或者结构方程模型中，内生变量是系统内的变量，都至少是一个外生变量的后代，外生变量不能是任何其他变量的后代，它们没有祖先，在图中表示为根节点。
#### 内生性与因果关系
当误差项与解释变量相关时，我们无法识别解释变量对被解释变量的因果关系，或者说最小二乘估计量是有偏的。
#### 内生性来源
- 遗漏变量（遗漏的变量纳入误差项，而这个遗漏的变量又与已知的解释变量相关）
- 逆向因果（X影响Y，Y也影响X）
- 测量误差
- 样本选择偏误
#### 工具变量
合格的工具变量Z满足的条件：
- 相关性<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Corr%28X%2C+Z%29+%5Cneq+0" 
alt="Corr(X, Z) \neq 0">
- 外生性<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+E%28Z+%5Cmid+u%29%3D0" 
alt="E(Z \mid u)=0">

为了克服逆向因果导致的内生性，可以考虑使用解释变量的时滞变量，比如2021供给影响2021价格，价格也会影响供给，我们可以使用2020年的价格作为工具变量，它与2021年价格高度相关，又不会和2021供给有关。但实际上，合格的工具变量难寻。
#### 两阶段最小二乘（2SLS）
两阶段最小二乘的基本思想是使用工具变量把<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">分成两个部分，一部分能被<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Z" 
alt="Z">解释的，外生的<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7BX%7D" 
alt="\hat{X}">，另一部分是造成问题的，内生的<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X-%5Chat%7BX%7D" 
alt="X-\hat{X}">；再将<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7BX%7D" 
alt="\hat{X}">作为外生的解释变量，代替<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">去解释<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Y" 
alt="Y">。以一个工具变量为例：
1. <img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">对<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Z" 
alt="Z">做回归，分离出<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">中与随机误差项<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+u" 
alt="u">不相关的部分：

    <img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X_%7Bi%7D%3D%5Cpi_%7B0%7D%2B%5Cpi_%7B1%7D+Z_%7Bi%7D%2Bv_%7Bi%7D" 
alt="X_{i}=\pi_{0}+\pi_{1} Z_{i}+v_{i}">

    因为<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Z" 
alt="Z">与<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+u" 
alt="u">不相关，所以![\pi_{0}+\pi_{1} Z_{i}](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cpi_%7B0%7D%2B%5Cpi_%7B1%7D+Z_%7Bi%7D)与<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+u_%7Bi%7D" 
alt="u_{i}">不相关。计算<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X" 
alt="X">的拟合值：

    <img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7BX%7D_%7Bi%7D%3D%5Chat%7B%5Cpi%7D_%7B0%7D%2B%5Chat%7B%5Cpi%7D_%7B1%7D+Z_%7Bi%7D" 
alt="\hat{X}_{i}=\hat{\pi}_{0}+\hat{\pi}_{1} Z_{i}">

    <img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X_%7Bi%7D%3D%5Chat%7BX%7D_%7Bi%7D%2B%5Chat%7Bv%7D_%7Bi%7D" 
alt="X_{i}=\hat{X}_{i}+\hat{v}_{i}">

2. 用<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7BX%7D_%7Bi%7D" 
alt="\hat{X}_{i}">替代<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X_%7Bi%7D" 
alt="X_{i}">，用<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+Y" 
alt="Y">对<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7BX%7D_%7Bi%7D" 
alt="\hat{X}_{i}">做回归

    <img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Baligned%7D%0A%26Y_%7Bi%7D%3D%5Cbeta_%7B0%7D%2B%5Cbeta_%7B1%7D+X_%7Bi%7D%2Bu_%7Bi%7D+%5C%5C%0A%26%3D%5Cbeta_%7B0%7D%2B%5Cbeta_%7B1%7D+%5Chat%7BX%7D_%7Bi%7D%2B%5Cbeta_%7B1%7D%5Cleft%28X_%7Bi%7D-%5Chat%7BX%7D_%7Bi%7D%5Cright%29%2Bu_%7Bi%7D+%5C%5C%0A%26%3D%5Cbeta_%7B0%7D%2B%5Cbeta_%7B1%7D+%5Chat%7BX%7D_%7Bi%7D%2B%5Cleft%28%5Cbeta_%7B1%7D+%5Chat%7Bv%7D_%7Bi%7D%2Bu_%7Bi%7D%5Cright%29%0A%5Cend%7Baligned%7D" 
alt="\begin{aligned}
&Y_{i}=\beta_{0}+\beta_{1} X_{i}+u_{i} \\
&=\beta_{0}+\beta_{1} \hat{X}_{i}+\beta_{1}\left(X_{i}-\hat{X}_{i}\right)+u_{i} \\
&=\beta_{0}+\beta_{1} \hat{X}_{i}+\left(\beta_{1} \hat{v}_{i}+u_{i}\right)
\end{aligned}">

    由于<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7BX%7D_%7Bi%7D" 
alt="\hat{X}_{i}">与<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+u_%7Bi%7D" 
alt="u_{i}">不相关，<img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7BX%7D_%7Bi%7D" 
alt="\hat{X}_{i}">与自己的残差也不相关，因此该方程符合线性回归的前提假设。
